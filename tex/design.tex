\section{Design}
\label{sec:design}

\subsection{Mainlines}

\Moca consist of a Linux kernel module and a script responsible to launching it
with the right parameters. It does not relies on architecture specific
technologies such as AMD IBS or Intel PEBS, or architecture dependent kernel
code. Therefore it is highly portable and can be run on any recent kernel
(Linux $> 3.X$).

In recent Linux kernel, memory page are not allocate before the execution: the
first access to a page will trigger a page fault. Linux will then allocate the
page. Page fault can also be triggered when a thread access a page modified by
an other thread. We can monitor memory access by registering a callback on
Linux page faults. Still page fault does not occurs at each memory access so
we need to generate false page fault by periodically marking some page as not
present. This method have several advantages over hardware sampling or
instrumentation. First it provides a super set of the accesses: indeed it
guarantee that for each page of the monitored application, at least one access
will be traced. Moreover we can still trace the exact accessed address
therefore the granularity is not limited while instrumentation based tools
often works at a high granularity to keep their overhead reasonable.  Finally
trapping to our tool has a very low cost as it relies on page faults which
are triggered by hardware and the Linux kernel probing mechanism:
\emph{kprobes}.

\subsection{Algorithms and technical aspects}
\label{sec:design-tech}

In this section we explain in more details how the main components of \Moca
interact and how we addressed some technical issues.

\begin{figure}[htb]
    \centering
    \resizebox{\linewidth}{!}{
        \Input{moca-tikz.tex}
    }
    \caption{Interactions betweens \Moca's main component and Linux}
    \label{fig:moca}
\end{figure}


During the execution, we need to store three types of information:
\begin{enumerate}
    \item Which \emph{tasks} (Linux internal representation of threads/processes) are
monitored (as page fault will be triggered by tasks which does not belong to
the monitored application).
    \item Which page faults have been triggered by \Moca (false page faults).
    \item The set of addresses recently accessed by each task.
\end{enumerate}

Each of these informations are stored on preallocated hash maps to reduce the
runtime overhead.  The two first are read during every page faults but rarely
written (when a new tasks of the monitored application triggers it first page
fault or when \Moca create some false page faults). Therefore they are
protected with Linux kernel built-in \emph{rwlocks}. The third is the actual
trace, each task has a private set of \emph{chunks}. A chunk is a set of
accesses that occurred in a (short) time laps.  Chunks provide a discrete
representation of the time and reduce the amount of information stored (only
two clocks per chunk). The discretization of time is useful as it allow the
different component of \Moca to work on different data.  Indeed the traced
program will always work on \emph{current} chunks, while the logging daemon
which flush the trace from memory to disc works on \emph{finished} chunks. A
monitoring (kernel) thread periodically wakes up, mark the current chunks as
\emph{ending} and invalidate every page of these chunks. Once all pages of the
chunk have been invalidated, it mark the chunk as \emph{fnished}. Finally, the
logging process flush each \emph{finished} chunks to the disc at a lower
frequency.  Figure~\ref{fig:moca} illustrate the interaction between the
different process and threads of \Moca, these data structures and Linux.

\Moca generate one (plain text) trace file per thread, each file contain three
types of information: meta data about the task (pid and \Moca internal id),
chunks related information (number of element in the chunk, start and end
clock) and accesses information (virtual and physical address, number of read
and writes). Moreover for both chunks and accesses a bitmask indicate on which
CPU the accesses were detected.

\begin{algorithm}[htb]
    \caption{Monitoring thread algorithm}
    \label{algo:monTh}
    \begin{algorithmic}[1]
        \While{\Callp{NotFinished}{}}
        \ForAll{t in \Callp{MonitoredTasks}{}}
        \State \Callp{EndCurrentChunk}{t}
                \ForAll{Addr in \Callp{PreviousChunk}{t}}
                    \State \Callp{WriteLockPF}{}
                    \State \Callp{AddFalsePF}{Addr}
                    \State \Callp{WriteUnlockPF}{}
                \EndFor
            \EndFor
            \State \Callp{sleep}{MonitorThreadWakeUpInterval}
        \EndWhile
    \end{algorithmic}
\end{algorithm}

The monitor thread (kernel thread, algorithm~\ref{algo:monTh}) is
responsible to generate false page faults by removing the \texttt{PRESENT}
flags from the \emph{Page Table Entry} (pte) corresponding to each recently
accessed addresses. The more page faults occurs, the more
precise the trace is. Still to invalidate the page, it needs
to take a write lock on the page faults hashmap. Therefore
delay every pending page fault. The
\texttt{MonitorThreadWakeUpInterval} parameter allows the user
to modify slightly this equilibrium.

\begin{algorithm}[htb]
    \caption{Logging daemon algorithm. Note that no locks are required as we
    work on finished chunks.}
    \label{algo:flushTh}
    \begin{algorithmic}[1]
        \While{\Callp{NotFinished}{}}
        \ForAll{t in \Callp{MonitoredTasks}{}}
                \ForAll{c in \Callp{FinishedChunks}{t}}
                \State \Callp{WriteTraceToDisk}{c}
                \State \Callp{ReinitChunk}{c}
                \EndFor
            \EndFor
            \State \Callp{sleep}{LoggingDaemonWakeupInterval}
        \EndWhile
    \end{algorithmic}
\end{algorithm}

The logging daemon (algorithm~\ref{algo:flushTh}) is a userspace process
which periodically reads \texttt{/proc} pseudo files. Those reads trigger a
callback method in our tool which flush finished cunks from memory to the
disc. As it works on finished chunks, it does not directly interfere with the
normal application execution. Still as it generate disc I/O, it is preferable to
keep it asleep most of the time. Yet if it does not wake up often enough, no
more chunks will be available to store accesses and a part of the trace will
be lost.

\begin{algorithm}[htb]
    \caption{Page fault handler}
    \label{algo:PageFault}
    \begin{algorithmic}[1]
        \Function{HandleFault}{task t,void *addr, int type}
            \If {\Callp{IsNotMonitoredTask}{t}}
                \If {!\Callp{AddToMonitoredIfNeeded}{t}}
                    \State \Return \Comment{Resume page fault}
                \EndIf
            \EndIf
            \State \Callp{AddToChunk}{t,addr, type}
            \Comment{Trace the access}
            \State \Callp{ReadLockPF}{}
            \State \Callp{TryFixFalsePageFault}{addr}
            \State \Callp{ReadUnlockPF}{}
            \State \Callp{UpdateClock}{}
            \State \Comment{Resume page fault. If a fix occurred, Linux will
                silently abort the page fault}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

Every time a page fault occurs, it traps to \Moca
(algorithm~\ref{algo:PageFault}), we first need to determine if the task
(thread or process) responsible for the page fault is
monitored. If not, we have to check if the task is a child of on monitored
task, in this case, we have to monitor it.We only need a read lock
to a hashmap containing the monitored tasks to answer these
two questions. If the answer is no, Linux will handle the page
fault normally. If and only if the second answer is yes, we will need to take
a write lock, this case occurs only at the first page fault of a new monitored
process or thread which is quite rare. For instance on the benchmarks used for
the evaluation it happens $8$ times for $10^6$ accesses.

When a monitored task triggers a page fault, we first add the accessed address
to it's current chunk. For each access, we store the address, it's type (read
or write) and the CPU on which the fault occurred.  Then we check if the page
fault was triggered by \Moca or if it is a legitimate page fault. In the first
case, we \emph{fix} it which mean setting the present flag on the \texttt{Page
Table Entry} and we mark the hash map entry \emph{BAD} so we only need a read
lock on the false page fault hashmap. \emph{BAD} entries are removed if
necessary by a call to \texttt{AddFalsePF} which already holds the write lock.
If a fix occurred during our handler, Linux will silently abort the page fault
without any more work, else it will execute a normal page fault. Each page
fault increase an atomic clock that is used to timestamp the beginning and end
of the chunks.

A race might occurs if the monitor threads clears a page fault between the
end of our handler and the end of Linux page fault. To avoid that, we store
for each CPU the last address that faulted, and we forbid to clear this
address.

Every settings that can influence either the accuracy of the trace or the
overhead of the tools (such as the number and size of chunks, wakeup intervals
for the monitoring thread and the logging process \ldots) are accessible to the
user although the default have been defined after an experimental study
(detailed in section~\ref{sec:expe-ovh}).

In the next section we evaluate \Moca by comparing it to existing tools in
term of performances and trace precision.
