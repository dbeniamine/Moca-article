\section{Design}
\label{sec:design}

\Moca consist of a Linux kernel module and a script that loads this module with
the proper parameters and launch the monitored application on the user behalf.
It does not rely on architecture specific
technologies such as AMD IBS or Intel PEBS, or architecture dependent kernel
code. Therefore it is highly portable and can be run on any recent Linux kernel
from the $3.0$. \Moca collects complete traces in the sense that the exact set
of pages accessed by the application is deduced from the collected events
at all times during the execution. Thus, it is complete at the page granularity.
Other information such as exact addresses and access times are a sample of
the set of all the accesses.

Two tasks are addressed by the kernel module included in \Moca. The first one is
keeping track of the set of pages accessed by the application during an elementary monitoring
interval. The second one is managing somehow the huge quantity of data produced
by the trace collection within the kernel space in-between regular flushes toward
the user space. Of course, these two tasks should be as slightly intrusive as possible.

\subsection{Collecting memory accesses}

In recent Linux kernel, physical memory pages are lazily allocated to page frames during
the execution. The first access to a page in the virtual address space triggers a page fault.
To handle this page fault, Linux allocates a physical page to the requested page frame.
Such a page fault can also be triggered when a thread access a shared page modified by
another thread. \Moca is built upon the possibility to monitor memory accesses by registering
a callback on Linux page faults.

Nevertheless, a page fault does not occur at each memory access. To monitor memory accesses during
the course of the execution, we need to reenable a page fault similar to the first access, but
performed on a regular basis and on behalf of \Moca.
In other words, we need to generate false page fault by periodically marking as \emph{not present}
the pages accesses by the application.
In Linux terminology, this means that any access to the page will trigger a page fault which
will have to be handled, in this case, by a handler contained in \Moca.

This method has several advantages over hardware sampling or
instrumentation. First it provides a superset of all the memory accesses, because it
guarantee that each page accessed by the monitored application will at least fault once
and will be traced. Thus, at the end of each monitoring interval, we know the exact set
of accessed pages from which we deduce a superset of actual memory accesses.
This comes in addition to the fact that each false page fault generated provides \Moca with
exact information about one memory access. This means that \Moca also performs a sampling
of all the memory accesses and, because it is designed to manage large chunks of trace data
within the kernel space, it stores all the details about these samples in the collected trace.

\Moca differs from instruction sampling because it is not necessary to increase the monitoring
frequency of \Moca to collect a complete trace. On the contrary, when using instruction sampling,
if the pages of the application are accessed in an unbalanced manner, it is necessary to increase
the sampling frequency to get a precise picture of the memory working set of the application.
Nevertheless, there can be no guarantee that a chosen sampling frequency will result in a trace
that contains all the pages on which the application works.
\Moca also differs from instrumentation based tools because, just as in the case of sampling,
memory accesses that are not collected in the trace are not trapped at all. Furthermore, the
remaining memory accesses, those which are collected, are trapped using a hardware mechanism and
Linux kernel probes. Both are lightweight mechanisms, this means that
the overall instrumentation overhead of \Moca is likely to be much lower.
Indeed, instrumentation based methods often work at a high granularity, collecting few information,
in order to restrain their naturally high overhead.

In short, \Moca collects complete traces at the granularity of the page, whereas instruction
sampling cannot guarantee any completeness of the collected trace. And \Moca is as lightweight
as instruction sampling whereas binary instrumentation has to trap all the memory instructions.
Furthermore \Moca traces provides information on time, thread sharing and CPU
location of each captured accesses. The \tbl{tab:tools-comp} summarizes these main differences between \Moca and the other memory profiling tools.

\begin{table}
    \centering
    \begin{tabular}{p{1.3cm}lC{2.5cm}C{2.5cm}}
        \toprule
        & & Granularity & superset \\
        \cmidrule(lr){3-4}
        \multirow{4}{.8cm}{Trace precision}
        & Tabarnac & Page & Page \\
        & Mitos & Address & None \\
        & MemProf & Address & None \\
        & Moca & Address & Page \\
        \midrule
        & & Time & Sharing and CPU localisation \\
        \cmidrule(lr){3-4}
        \multirow{4}{.8cm}{Other information}
        & Tabarnac & No & Thread sharing\\
        \addlinespace
        & Mitos & Yes & CPU location \\
        \addlinespace
        & MemProf & Yes & Thread sharing and CPU location \\
        \addlinespace
        & Moca & Yes & Thread sharing and CPU location \\
        \midrule
        & & Collection method & Architecture \\
        \cmidrule(lr){3-4}
        & Tabarnac & Instrumentation & Intel, AMD \\
        \addlinespace
        \multirow{4}{.8cm}{Portability}
        & Mitos & Hardware sampling and Instrumentation & Intel, AMD, IBM \\
        \addlinespace
        & MemProf & Hardware sampling & AMD \\
        \addlinespace
        & Moca & Complete sampling & Any architecture\\
        \bottomrule
    \end{tabular}
    \caption{Comparison of different memory accesses collection
        tools: Tabarnac~\cite{Beniamine15TABARNACRR},
        Mitos~\cite{Gimenez14Dissecting},
        MemProf~\cite{Lachaize12MemProf} and \Moca}
        \label{tab:tools-comp}
\end{table}

%Intercepting page faults instead of doing hardware sampling, guaranty that
%every page of the analyzed application will be intercepted. Therefore \Moca
%provide a \emph{complete} trace at the granularity of the page. Such guaranty
%is not provided by any other tool based on sampling. Instrumentations based
%tools that traces every memory accesses provides this guaranty but to keep
%their overhead reasonable they have to drop a part of the information.
%Furthermore while \Moca traces are \emph{complete} at the page granularity,
%they contains every intercepted addresses, thus they provide detailed
%information at the byte granularity.

\subsection{Managing data}
\label{sec:design-tech}

In this section we present in details how the main components of \Moca
interact and how \Moca addresses the management of the data it collects
within the kernel space.

\begin{figure}[htb]
    \centering
    \resizebox{\linewidth}{!}{
        \Input{moca-tikz.tex}
    }
    \caption{Interactions betweens \Moca's main component and Linux}
    \label{fig:moca}
\end{figure}


During the execution, \Moca needs to store three types of information:
\begin{enumerate}
    \item the set of \emph{tasks} (Linux internal representation of threads and process) which are
monitored. This is necessary because page faults will also be triggered by other tasks which does not belong to
the monitored application.
    \item the set of addresses recently accessed by each task. This is necessary to re-enable the false page
      fault at the end of each monitoring interval.
    \item the set of all page faults which have been triggered by \Moca. They will be stored until they are
      transfered to the user space by a dedicated thread.
\end{enumerate}

The first two types of information are stored in preallocated hashmaps in order to reduce the
runtime overhead of their management.  The two first hashmaps are read at each page fault but rarely
written, only when a new task of the monitored application triggers it first page
fault or when \Moca create some false page faults. We can protect
them with Linux kernel built-in \emph{rwlocks}.\GH{Un peu contradictoire si on sait que les rwlocks favorisent les lecteurs\ldots}
The third type of information is the actual
trace, divided, for each task, in a private set of \emph{chunks}. A chunk is the set of
accesses that have been collected during some configurable time interval. Chunks provide a discretization
of the time, that we use as a timestamp for all the accesses it contains. This is not a limitation as timestamps more
precise than the sampling frequency would not really make sense. The advantage is that it reduces the volume of information stored for the accesses.

This discretization of the time, materialized as a sequence of chunks, is useful as it let the
different components of \Moca work concurrently on different chunks.  Indeed the traced
program will always work on \emph{current} chunks, one for each core, while the logging daemon,
which flushes the trace from memory to disc works on \emph{completed} chunks. A
monitoring kernel thread, manage the progressing of this logical time. It periodically wakes up, marks the current chunks as
\emph{ending} and invalidate all the pages they reference. Once all pages of the \emph{ending}
chunks have been invalidated, it marks the chunk as \emph{finished}. Finally, the
logging process flush \emph{finished} chunks to the filesystem at a lower
frequency, and recycle them as empty places for upcoming chunks.  \fig{fig:moca} depicts the interaction between the
different processes and threads of \Moca, its data structures and Linux.

At the end of the day, \Moca generates one plain text trace file per thread. Each of these files contains three
types of information: meta data about the task (its pid and \Moca internal id),
chunks related information (their number of elements, their start and end
clock) and information on each intercepted accesses (virtual and physical addresses, number of read
and writes \GH{N'y a-t-il pas une entrée par accès ???}\DB{Si, j'ai fait une
petite modif pour clarifier}). Moreover for both chunks and accesses a bitmask indicate on which
CPU the accesses have been detected.

\begin{algorithm}[htb]
    \caption{Monitoring thread algorithm}
    \label{algo:monTh}
    \begin{algorithmic}[1]
        \While{\Callp{NotFinished}{}}
            \ForAll{t in \Callp{MonitoredTasks}{}}
                \State \Callp{EndCurrentChunk}{t}
                    \ForAll{Addr in \Callp{PreviousChunk}{t}}
                        \State \Callp{WriteLockPF}{}
                        \State \Callp{AddFalsePF}{Addr}
                        \State \Callp{WriteUnlockPF}{}
                    \EndFor
                \State \Callp{MarkPreviousChunkFinished}{t}
            \EndFor
            \State \Callp{sleep}{MonitorThreadWakeUpInterval}
        \EndWhile
    \end{algorithmic}
\end{algorithm}

The monitoring thread, which is a kernel thread that uses the algorithm~\ref{algo:monTh}, is
in charge of enabling false page faults destined for \Moca. It performs its task by
removing the \texttt{PRESENT} flags from the \texttt{Page Table Entry} (PTE) that corresponds to each recently
accessed addresses. Of course, the more frequently this thread can enable page faults, the more
precise the trace is. This frequency is the sampling frequency of our tools, which is able to collect
one access per distinct page between two wakeup of the monitoring thread.
But invalidating all the recently accessed pages takes time and requires
to take a write lock on the page faults hashmap. This write lock
delays any pending false page fault in the monitored application. Thus the wakeup frequency of the
monitoring thread cannot be too high, otherwise its action becomes too intrusive.
The \texttt{MonitorThreadWakeUpInterval} \Moca parameter lets the user change the default setting
that we have empirically chosen.

\begin{algorithm}[htb]
    \caption{Logging daemon algorithm. Note that no locks are required to
    work on finished chunks.}
    \label{algo:flushTh}
    \begin{algorithmic}[1]
        \While{\Callp{NotFinished}{}}
        \ForAll{t in \Callp{MonitoredTasks}{}}
                \ForAll{c in \Callp{FinishedChunks}{t}}
                \State \Callp{WriteTraceToDisk}{c}
                \State \Callp{ReinitChunk}{c}
                \EndFor
            \EndFor
            \State \Callp{sleep}{LoggingDaemonWakeupInterval}
        \EndWhile
    \end{algorithmic}
\end{algorithm}

The logging daemon, that uses algorithm~\ref{algo:flushTh}, is a userspace process
which periodically reads \texttt{/proc} pseudo files used by \Moca kernel module to export
its data to userspace. Those reads trigger a
callback method in our tool which flush finished chunks from memory to
disc. As it works on finished chunks, it does not directly interfere with the
normal application execution. Especially, no lock is required to access to these finished chunks,
and, as it mostly generates disc I/O, it does not compete much for CPU.
\Moca has just to wake him up sufficiently often so that the kernel module does not run out
of free space to store upcoming chunks.

\begin{algorithm}[htb]
    \caption{Page fault handler}
    \label{algo:PageFault}
    \begin{algorithmic}[1]
        \Function{HandleFault}{task t,void *addr, int type}
            \If {\Callp{IsNotMonitoredTask}{t}}
                \If {!\Callp{AddToMonitoredIfNeeded}{t}}
                    \State \Return \Comment{Resume page fault}
                \EndIf
            \EndIf
            \State \Callp{AddToChunk}{t,addr, type}
            \Comment{Trace the access}
            \State \Callp{ReadLockPF}{}
            \State \Callp{TryFixFalsePageFault}{addr}
            \State \Callp{ReadUnlockPF}{}
            \State \Callp{UpdateClock}{}
            \State \Comment{Resume page fault. If a fix occurred, Linux will
                silently abort the page fault}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

Each time a page fault occurs, it is trapped by the \Moca handler that uses
algorithm~\ref{algo:PageFault}. This algorithm first finds out if the task
(thread or process) responsible for the page fault is
monitored or not. If not, it has to check if the task is a child of a monitored
task, in this case, we have to monitor it as well. We only need a read lock
to access to the hashmap containing the monitored tasks in order to answer to these
two questions.
If and only if the second answer is yes, it is necessary to take
a write lock to add the task to the monitored ones.
This case occurs only at the first page fault of a new monitored
process or thread which is quite rare. For instance, in the benchmarks used for
the evaluation it happens $8$ times out of $10^6$ accesses.
At the end of this phase, if the task is still not monitored, Linux can handle the page fault normally.

When a monitored task triggers a page fault, the access is first added
to its current chunk. For each access, \Moca stores the exact address, it's type (read
or write) and the CPU on which the fault occurred.  Then, it checks if the page
fault was triggered by \Moca or if this is a legitimate page fault.

In the first
case, \Moca \emph{fixes} it by setting the present flag on the \texttt{PTE}.
The hashmap entry indicating that the fault was triggered by \Moca should then
be removed, but this would required a write lock, so we only mark the hashmap
entry as \emph{BAD}. \emph{BAD} entries are removed if required when the
monitor thread calls \texttt{AddFalsePF} as this function already holds the
write lock.

If a fix occurred in the \Moca handler, Linux silently aborts the page fault
when it resumes its execution. In the other case, it executes a normal page fault handling. Each page
fault increases an atomic clock that is used to timestamp the beginning and end
of the chunks.
In this description, one can notice that a race might occur if the monitoring thread enables a false page fault between the
end of our handler and the end of Linux page fault handler. To avoid that, \Moca stores
for each CPU the last address that faulted, and does not clear it.

All the settings that can influence either the accuracy of the trace or the
overhead of the tools (such as the number and size of chunks, wakeup intervals
for the monitoring thread and the logging process \ldots) can be overridden by the
user. Nevertheless, reasonable defaults have been defined from the experimental study
detailed in section~\ref{sec:expe-ovh}.
