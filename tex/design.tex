\section{Design}
\label{sec:design}

\Moca consist of a Linux kernel module and a script that loads this module with
the proper parameters and launch the monitored application on the user behalf.
It does not rely on architecture specific
technologies such as AMD IBS or Intel PEBS, or architecture dependent kernel
code. Therefore it is highly portable and can be run on any recent Linux kernel
from the 3.0. \Moca collects complete traces in the sense that the exact set
of pages accessed by the application is deduced from the collected events
at all times during the execution. Thus, it is complete at the page granularity.
Other information such as exact addresses and access times are a sample of
the set of all the accesses.

Two tasks are addressed by the kernel module included in \Moca. The first one is
keeping track of the set of pages accessed by the application during an elementary monitoring
interval. The second one is managing somehow the huge quantity of data produced
by the trace collection whithin the kernel space inbeetween regular flushes toward
the user space. Of course, these two tasks should be as slightly intrusive as possible.

\subsection{Collecting memory accesses}

In recent Linux kernel, physical memory pages are lazily allocated to page frames during
the execution. The first access to a page in the virtual address space triggers a page fault.
To handle this page fault, Linux allocates a physical page to the requested page frame.
Such a page fault can also be triggered when a thread access a shared page modified by
another thread. \Moca is built upon the possibility to monitor memory accesses by registering
a callback on Linux page faults.

Nevertheless, a page fault does not occur at each memory access. To monitor memory accesses during
the course of the execution, we need to reenable a page fault similar to the first access, but
performed on a regular basis and on behalf of \Moca.
In other words, we need to generate false page fault by periodically marking as \emph{not present}
the pages accesses by the application.
In Linux terminology, this means that any access to the page will trigger a page fault which
will have to be handled, in this case, by a handler contained in \Moca.

This method has several advantages over hardware sampling or
instrumentation. First it provides a superset of all the memory accesses, because it
guarantee that each page accessed by the monitored application will at least fault once
and will be traced. Thus, at the end of each monitoring interval, we know the exact set
of accessed pages from which we deduce a superset of actual memory accesses.
This comes in addition to the fact that each false page fault generated provides \Moca with
exact information about one memory access. This means that \Moca also performs a sampling
of all the memory accesses and, because it is designed to manage large chunks of trace data
within the kernel space, it stores all the details about these samples in the collected trace.

\Moca differs from instruction sampling because it is not necessary to increase the monitoring
frequency of \Moca to collect a complete trace. On the contrary, when using instruction sampling,
if the pages of the application are accessed in an unbalanced manner, it is necessary to increase
the sampling frequency to get a precise picture of the memory working set of the application.
Nevertheless, there can be no guarantee that a chosen sampling frequency will result in a trace
that contains all the pages on which the application works.
\Moca also differs from instrumentation based tools because, just as in the case of sampling,
memory accesses that are not collected in the trace are not traped at all. Furthermore, the
remaining memory accesses, those which are collected, are traped using a hardware mechanism and
Linux kernel probes. Both are lightweight mechanisms, this means that
the overall instrumentation overhead of \Moca is likely to be much lower.
Indeed, instrumentation based methods often work at a high granularity, collecting few information,
in order to restrain their naturally high overhead.

In short, \Moca collects complete traces at the granularity of the page, whereas instruction
sampling cannot guarantee any completeness of the collected trace. And \Moca is as lightweight
as instruction sampling whereas binary instrumentation has to trap all the memory instructions.
The table \tbl{tab:tools-comp} summarizes these main differences between \Moca and the other memory profiling tools.

\begin{table}
    \centering
    \begin{tabular}{p{1.3cm}lC{2.5cm}C{2.5cm}}
        \toprule
        & & Granularity & superset \\
        \cmidrule(lr){3-4}
        \multirow{4}{.8cm}{Trace precision}
        & Tabarnac & Page & Page \\
        & Mitos & Address & None \\
        & MemProf & Address & None \\
        & Moca & Address & Page \\
        \midrule
        & & Time & Sharing and CPU localisation \\
        \cmidrule(lr){3-4}
        \multirow{4}{.8cm}{Other information}
        & Tabarnac & No & Thread sharing\\
        \addlinespace
        & Mitos & Yes & CPU location \\
        \addlinespace
        & MemProf & Yes & Thread sharing and CPU location \\
        \addlinespace
        & Moca & Yes & Thread sharing and CPU location \\
        \midrule
        & & Collection method & Architecture \\
        \cmidrule(lr){3-4}
        & Tabarnac & Instrumentation & Intel, AMD \\
        \addlinespace
        \multirow{4}{.8cm}{Portability}
        & Mitos & Hardware sampling and Instrumentation & Intel, AMD, IBM \\
        \addlinespace
        & MemProf & Hardware sampling & AMD \\
        \addlinespace
        & Moca & Complete sampling & Any architecture\\
        \bottomrule
    \end{tabular}
    \caption{Comparison of different memory accesses collection
        tools: Tabarnac~\cite{Beniamine15TABARNACRR},
        Mitos~\cite{Gimenez14Dissecting},
        MemProf~\cite{Lachaize12MemProf} and \Moca}
        \label{tab:tools-comp}
\end{table}

%Intercepting page faults instead of doing hardware sampling, guaranty that
%every page of the analyzed application will be intercepted. Therefore \Moca
%provide a \emph{complete} trace at the granularity of the page. Such guaranty
%is not provided by any other tool based on sampling. Instrumentations based
%tools that traces every memory accesses provides this guaranty but to keep
%their overhead reasonable they have to drop a part of the information.
%Furthermore while \Moca traces are \emph{complete} at the page granularity,
%they contains every intercepted addresses, thus they provide detailed
%information at the byte granularity.

\subsection{Managing data}
\label{sec:design-tech}

In this section we present in details how the main components of \Moca
interact and how \Moca addresses the management of the data it collects
within the kernel space.

\begin{figure}[htb]
    \centering
    \resizebox{\linewidth}{!}{
        \Input{moca-tikz.tex}
    }
    \caption{Interactions betweens \Moca's main component and Linux}
    \label{fig:moca}
\end{figure}


During the execution, \Moca needs to store three types of information:
\begin{enumerate}
    \item the set of \emph{tasks} (Linux internal representation of threads and process) which are
monitored. This is necessary because page faults will also be triggered by other tasks which does not belong to
the monitored application.
    \item the set of addresses recently accessed by each task. This is necessary to re-enable the false page
      fault at the end of each monitoring interval.
    \item the set of all page faults which have been triggered by \Moca. They will be stored until they are
      transfered to the user space by a dedicated thread.
\end{enumerate}

The first two types of information are stored in preallocated hashmaps in order to reduce the
runtime overhead of their management.  The two first hashmaps are read at each page fault but rarely
written, only when a new task of the monitored application triggers it first page
fault or when \Moca create some false page faults. We can protect
them with Linux kernel built-in \emph{rwlocks}.\GH{Un peu contradictoire si on sait que les rwlocks favorisent les lecteurs\ldots}
The third type of information is the actual
trace, divided, for each task, in a private set of \emph{chunks}. A chunk is the set of
accesses that have been collected during some configurable time interval. Chunks provide a discretization
of the time and reduce the volume of information stored (only
two clocks per chunk).\GH{C'est quoi ces clocks ? Si c'est stocké une fois par chunk, la réduction de volume est ridicule}

This discretization of the time, materialized as a sequence of chunks, is useful as it let the
different components of \Moca work concurrently on different chunks.  Indeed the traced
program will always work on \emph{current} chunks, one for each core, while the logging daemon,
which flushes the trace from memory to disc works on \emph{completed} chunks. A
monitoring kernel thread, manage the progressing of this logical time. It periodically wakes up, marks the current chunks as
\emph{ending} and invalidate all the pages they reference. Once all pages of the \emph{ending}
chunks have been invalidated, it marks the chunk as \emph{finished}. Finally, the
logging process flush \emph{finished} chunks to the filesystem at a lower
frequency, and recycle them as empty places for upcoming chunks.  \fig{fig:moca} depicts the interaction between the
different processes and threads of \Moca, its data structures and Linux.

At the end of the day, \Moca generates one plain text trace file per thread. Each of these files contains three
types of information: meta data about the task (its pid and \Moca internal id),
chunks related information (their number of elements, their start and end
clock) and accesses information (virtual and physical addresses, number of read
and writes\GH{N'y a-t-il pas une entrée par accès ???}). Moreover for both chunks and accesses a bitmask indicate on which
CPU the accesses have been detected.

\begin{algorithm}[htb]
    \caption{Monitoring thread algorithm}
    \label{algo:monTh}
    \begin{algorithmic}[1]
        \While{\Callp{NotFinished}{}}
            \ForAll{t in \Callp{MonitoredTasks}{}}
                \State \Callp{EndCurrentChunk}{t}
                    \ForAll{Addr in \Callp{PreviousChunk}{t}}
                        \State \Callp{WriteLockPF}{}
                        \State \Callp{AddFalsePF}{Addr}
                        \State \Callp{WriteUnlockPF}{}
                    \EndFor
                \State \Callp{MarkPreviousChunkFinished}{t}
            \EndFor
            \State \Callp{sleep}{MonitorThreadWakeUpInterval}
        \EndWhile
    \end{algorithmic}
\end{algorithm}

The monitor thread (kernel thread, algorithm~\ref{algo:monTh}) is
responsible to generate false page faults by removing the \texttt{PRESENT}
flags from the \texttt{Page Table Entry} (pte) corresponding to each recently
accessed addresses. The more page faults occurs, the more
precise the trace is. Still to invalidate the page, it needs
to take a write lock on the page faults hashmap. Therefore
delay every pending page fault. The
\texttt{MonitorThreadWakeUpInterval} parameter allows the user
to modify slightly this equilibrium.

\begin{algorithm}[htb]
    \caption{Logging daemon algorithm. Note that no locks are required as we
    work on finished chunks.}
    \label{algo:flushTh}
    \begin{algorithmic}[1]
        \While{\Callp{NotFinished}{}}
        \ForAll{t in \Callp{MonitoredTasks}{}}
                \ForAll{c in \Callp{FinishedChunks}{t}}
                \State \Callp{WriteTraceToDisk}{c}
                \State \Callp{ReinitChunk}{c}
                \EndFor
            \EndFor
            \State \Callp{sleep}{LoggingDaemonWakeupInterval}
        \EndWhile
    \end{algorithmic}
\end{algorithm}

The logging daemon (algorithm~\ref{algo:flushTh}) is a userspace process
which periodically reads \texttt{/proc} pseudo files. Those reads trigger a
callback method in our tool which flush finished chunks from memory to
disc. As it works on finished chunks, it does not directly interfere with the
normal application execution. Still as it generate disc I/O, it is preferable to
keep it asleep most of the time. Yet if it does not wake up often enough, no
more chunks will be available to store accesses and a part of the trace will
be lost.

\begin{algorithm}[htb]
    \caption{Page fault handler}
    \label{algo:PageFault}
    \begin{algorithmic}[1]
        \Function{HandleFault}{task t,void *addr, int type}
            \If {\Callp{IsNotMonitoredTask}{t}}
                \If {!\Callp{AddToMonitoredIfNeeded}{t}}
                    \State \Return \Comment{Resume page fault}
                \EndIf
            \EndIf
            \State \Callp{AddToChunk}{t,addr, type}
            \Comment{Trace the access}
            \State \Callp{ReadLockPF}{}
            \State \Callp{TryFixFalsePageFault}{addr}
            \State \Callp{ReadUnlockPF}{}
            \State \Callp{UpdateClock}{}
            \State \Comment{Resume page fault. If a fix occurred, Linux will
                silently abort the page fault}
        \EndFunction
    \end{algorithmic}
\end{algorithm}

Every time a page fault occurs, it traps to \Moca
(algorithm~\ref{algo:PageFault}), we first need to determine if the task
(thread or process) responsible for the page fault is
monitored. If not, we have to check if the task is a child of a monitored
task, in this case, we have to monitor it. We only need a read lock
to a hashmap containing the monitored tasks to answer these
two questions. If the answer is no, Linux will handle the page
fault normally. If and only if the second answer is yes, we will need to take
a write lock. This case occurs only at the first page fault of a new monitored
process or thread which is quite rare. For instance on the benchmarks used for
the evaluation it happens $8$ times for $10^6$ accesses.

When a monitored task triggers a page fault, we first add the accessed address
to it's current chunk. For each access, we store the address, it's type (read
or write) and the CPU on which the fault occurred.  Then we check if the page
fault was triggered by \Moca or if it is a legitimate page fault. In the first
case, we \emph{fix} it which mean setting the present flag on the \texttt{Page
Table Entry} and we mark the hash map entry \emph{BAD} so we only need a read
lock on the false page fault hashmap. \emph{BAD} entries are removed if
necessary by a call to \texttt{AddFalsePF} which already holds the write lock.
If a fix occurred during our handler, Linux will silently abort the page fault
without any more work, else it will execute a normal page fault. Each page
fault increase an atomic clock that is used to timestamp the beginning and end
of the chunks.

A race might occurs if the monitor threads clears a page fault between the
end of our handler and the end of Linux page fault. To avoid that, we store
for each CPU the last address that faulted, and we forbid to clear this
address.

Every settings that can influence either the accuracy of the trace or the
overhead of the tools (such as the number and size of chunks, wakeup intervals
for the monitoring thread and the logging process \ldots) are accessible to the
user although the default have been defined after an experimental study
(detailed in section~\ref{sec:expe-ovh}).

In the next section we evaluate \Moca by comparing it to existing tools in
term of performances and trace precision.
