\section{Conclusions}
\label{sec:cncl}

In this study, we addressed the issue of memory accesses collection for
multithreaded applications. This is a key challenge in high performance
computing as memory is often a performance
bottleneck. Memory traces can be used at runtime to improve data locality or
offline by developers to understand and improve the memory
behavior of their applications and, therefore, their performances. For online analysis the trace \emph{precision}
is limited by the volume of data that can be analyzed in real time, but for
offline usage, highly accurate traces can provides a better understanding of
the application memory behavior.

To address this challenge, we propose \Moca an efficient tool for the collection of \emph{precise},
\emph{complete} and \emph{detailed} memory traces. While existing tools
rely on \emph{incomplete} hardware sampling to
provide such traces at an acceptable cost, \Moca provides a \emph{complete}
trace, that contains all the accessed areas, at the granularity of the page.
Moreover, \Moca traces not only
contain all the pages that are accessed during the execution, but also, 
for each trapped access, temporal, spatial and sharing
information: accesses are timestamped and recorded along with their thread number, CPU number, and kind.
While \Moca works at the page granularity, it stores the exact
address of each intercepted accesses. Therefore, it also provides an
\emph{incomplete} trace at the granularity of the Byte, similar to
traces collected by instructions sampling. Furthermore, \Moca is also able to relate accesses to
data structures of the application by combining this efficient trace collection system with an examination
of the application binary.

Most state of the art tools are relying on hardware technologies such as Intel PEBS
or AMD IBS, and embed vendor (or processor) dependent code making them hard
to maintain and not portable. On the contrary, \Moca is based on page
faults interception as well as false page faults injection mechanisms and does not use any architecture dependent code.
It can work on any Linux kernel from $3.0$ only by loading a module and
without any kernel modification.

Several tools uses page faults interception to retrieve information about the memory
use. As information provided by only intercepting regular page faults
is not always \emph{precise} enough, a few tools also inject false page faults
on a regular basis to increase the trace \emph{precision}. To our knowledge, all the
existing tools relying on these mechanisms uses the collected data online and,
thus, do not have to manage and store a large volume of data. \Moca is the
first tool able to generate and store \emph{complete} and \emph{precise} memory traces for offline
analysis.

We evaluated \Moca by comparing it to two state of the art tools: \Mitos and
\MemProf (both with their default parameters and with some fine tunning of our own). We also compared it to a
tool from our previous contribution, \TABARNAC. For this comparison, we evaluated
two criteria: the \emph{precision} of the trace and the runtime overhead. We ran our
evaluation on the \NPB which are representative of multiple kinds of applications from simple kernels
to realistic ones. Our evaluation has exposed the fact that the tools
relying on hardware sampling miss a large part of the address space. It
has also shown that \Moca is able to provide both a \emph{complete} trace at the page
granularity and a sampling at the Byte granularity significantly more \emph{precise} than the
other tools. Generating comparable traces using \MemProf or \Mitos would
require to sample more memory instructions than the hardware can.
Finally, \Moca overhead is more important than the overhead of sampling
based tools but usually lower than the one induced by binary instrumentation.
% Déjà dit plus haut
%Moreover, adding the Pin instrumentation to retrieve date structures
%informations does not impact significantly this overhead.

Future work will focus on the visualization and exploitation of these memory traces
which is another challenge mainly due to the volume of the collected data.
